{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based Association Pattern mining to find the sumarized the terms and condition\n",
    "Team members:\n",
    "Nikhil Gola(MT18129) Ridha Juneja(MT18009) Saru Brar(MT18014) Yogesh Pandey(MT18140)\n",
    "*****************************************************************************************\n",
    "\n",
    "Importing all the necessary libraries required for the Rule based Association Pattern mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the min_support and min_confidence value for the Confidence table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support=0.4\n",
    "min_confidence=0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the pickle word dictionary pickle file after preprocessing of the data (pickle file will act as a cache to our System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordDictionaryPickle():\n",
    "    f=open('Pickled Data/word_Dictionary.pickle','rb')\n",
    "    vocab=pickle.load(f)\n",
    "    f.close()\n",
    "    return vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the word frequncy pickle file from preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordFrequencyPickle():\n",
    "    f1=open('Pickled Data/word_Frequency.pickle','rb')\n",
    "    DocVocab=pickle.load(f1)\n",
    "    #print(DocVocab['Facebook'])\n",
    "    f1.close()\n",
    "    return DocVocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the unique word pickle from the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadUniqueWordPickle(): \n",
    "    f1=open('Pickled Data/unique_Wordset.pickle','rb')\n",
    "    unique_word=pickle.load(f1)\n",
    "    unique_word=list(unique_word)\n",
    "    f1.close()\n",
    "    return unique_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word frequency in the training set(corpus) and calculating the probability of word for occuring in corpus and the tfidf values for every word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWordFrequency(DocVocab):\n",
    "    Word_Set_frequency={}\n",
    "    wordSetSupport = {}\n",
    "    tfidf_dict={}\n",
    "    maxDoc=len(DocVocab.keys())\n",
    "    print(maxDoc)\n",
    "    for key in DocVocab.keys():\n",
    "        for word in DocVocab[key]:\n",
    "            if Word_Set_frequency.get(word) ==None:\n",
    "                Word_Set_frequency[word]=DocVocab[key][word]\n",
    "                if DocVocab[key][word]==0:\n",
    "                    wordSetSupport[word]=0.0\n",
    "                else:\n",
    "                    wordSetSupport[word]=1.0\n",
    "            else:\n",
    "                if DocVocab[key][word]==0:\n",
    "                    wordSetSupport[word]+=0.0\n",
    "                else:\n",
    "                    wordSetSupport[word]+=1.0\n",
    "                Word_Set_frequency[word]+=DocVocab[key][word]\n",
    "    #print(wordSetSupport['combine'])\n",
    "    total1_sum=np.sum([Word_Set_frequency[key] for key in Word_Set_frequency.keys()])\n",
    "    for key in wordSetSupport.keys():\n",
    "            wordSetSupport[key]=wordSetSupport[key]/maxDoc\n",
    "            tfidf_dict[key]=wordSetSupport[key]*(Word_Set_frequency[key]/(1.0*total1_sum))\n",
    "    return Word_Set_frequency,wordSetSupport,tfidf_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the words from the wordset based upon their frequency which above the mean of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print([Word_Set_frequency[key]  for key in Word_Set_frequency.keys()])\n",
    "\n",
    "#print(mean1)\n",
    "#median1=np.median([Word_Set_frequency[key]  for key in Word_Set_frequency.keys()])\n",
    "#print(median1)\n",
    "#threshold=np.array([Word_Set_frequency[key]  for key in Word_Set_frequency.keys()])\n",
    "#threshold1=np.reshape(len(threshold),1)\n",
    "#kmeans = KMeans(n_clusters=4, random_state=0).fit(threshold1)\n",
    "#print(kmeans)\n",
    "def filterWordSetFrequencySupport(mean1,Word_Set_frequency,wordSetSupport):\n",
    "    req_Dic={}\n",
    "    req_Dic_Sup={}\n",
    "    for key in Word_Set_frequency.keys():\n",
    "        if Word_Set_frequency[key]>mean1:\n",
    "            req_Dic[key]=Word_Set_frequency[key]\n",
    "    for key in req_Dic.keys():\n",
    "        req_Dic_Sup[key]=wordSetSupport[key]\n",
    "    return req_Dic,req_Dic_Sup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fp = open('TFIDF_values.pickle','rb')\\ntfidf_values={}\\ntfidf = pickle.load(fp)\\n#print(tfidf['Google'])\\nfor key in tfidf.keys():\\n    for wordkey in tfidf[key].keys():\\n        if tfidf_values.get(wordkey)==None:\\n            tfidf_values[wordkey]=tfidf[key][wordkey]\\n        else:\\n            tfidf_values[wordkey]+=tfidf[key][wordkey]\\nfor key in tfidf_values.keys():\\n    tfidf_values[key] = tfidf_values[key]/len(tfidf.keys())\\n#print(tfidf_values[''])\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''fp = open('TFIDF_values.pickle','rb')\n",
    "tfidf_values={}\n",
    "tfidf = pickle.load(fp)\n",
    "#print(tfidf['Google'])\n",
    "for key in tfidf.keys():\n",
    "    for wordkey in tfidf[key].keys():\n",
    "        if tfidf_values.get(wordkey)==None:\n",
    "            tfidf_values[wordkey]=tfidf[key][wordkey]\n",
    "        else:\n",
    "            tfidf_values[wordkey]+=tfidf[key][wordkey]\n",
    "for key in tfidf_values.keys():\n",
    "    tfidf_values[key] = tfidf_values[key]/len(tfidf.keys())\n",
    "#print(tfidf_values[''])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def returnItemsWithMinSupport(vocab):\\n        maxDoc=len(vocab.keys())\\n        Support_Score={}\\n        for key in vocab.keys():\\n            for words in vocab[key]:\\n                if Support_Score.get(words) is None:\\n                    Support_Score[words]=1\\n                else:\\n                    Support_Score[words]+=1\\n        for key in Support_Score.keys():\\n            Support_Score[key]=Support_Score[key]/maxDoc\\n        return Support_Score\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def returnItemsWithMinSupport(vocab):\n",
    "        maxDoc=len(vocab.keys())\n",
    "        Support_Score={}\n",
    "        for key in vocab.keys():\n",
    "            for words in vocab[key]:\n",
    "                if Support_Score.get(words) is None:\n",
    "                    Support_Score[words]=1\n",
    "                else:\n",
    "                    Support_Score[words]+=1\n",
    "        for key in Support_Score.keys():\n",
    "            Support_Score[key]=Support_Score[key]/maxDoc\n",
    "        return Support_Score\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    words=[]\n",
    "    with open(\"data1.txt\",\"r\") as f:\n",
    "        x  = f.readlines()\n",
    "        for word in x:\n",
    "            words.append(word.replace(\"\\n\",\"\"))\n",
    "            \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterDictionary(A,Support_Score,unique_word):\n",
    "    filteredDict=[]\n",
    "    SScore={}\n",
    "    for word in A:\n",
    "        if word in unique_word:\n",
    "            filteredDict.append(word)\n",
    "            SScore[word]=Support_Score[word]\n",
    "    return filteredDict,SScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemmatizeWord(A):\n",
    "    WN=WordNetLemmatizer()\n",
    "    i=0;\n",
    "    maxLen=len(A)\n",
    "    while i<maxLen:\n",
    "        A[i]=WN.lemmatize(A[i])\n",
    "        i+=1\n",
    "    return A\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListToDic(vocab):\n",
    "    my_dict={}\n",
    "    for doc in vocab.keys():\n",
    "        my_dict = {k: 0 for k in vocab[doc] }\n",
    "        vocab[doc]=my_dict\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(A,B,vocab):\n",
    "    ScoreMatrix=np.zeros((len(A),len(B)))\n",
    "    #print(len(B))\n",
    "    #print(len(A))\n",
    "    for index1,word1 in enumerate(A):# A is list of words\n",
    "        for index2,word2 in enumerate(B):\n",
    "            common=0\n",
    "            totA=0\n",
    "            for doc in vocab.keys():\n",
    "                if  vocab[doc].get(word1) is not None :\n",
    "                    totA+=1\n",
    "                    if vocab[doc].get(word2) is not None:\n",
    "                        common+=1\n",
    "                    \n",
    "            if totA==0:\n",
    "                print(word1,word2)\n",
    "            ScoreMatrix[index1][index2]=common/totA\n",
    "    return ScoreMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(len(A))\n",
    "#print(requiredDicti['software'])\n",
    "#print('software' in B )\n",
    "#print(len(B))\n",
    "#print(B)\n",
    "#print(Ascore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectAssociationRules(confidenceMatrix,min_support,min_confidence,A,B,Support_Score):\n",
    "    row,col=confidenceMatrix.shape\n",
    "    #print(row,col)\n",
    "    required={}\n",
    "    for i in range(0,row):\n",
    "        for j in range(0,col):\n",
    "            word=A[i]\n",
    "            score=Support_Score[word]\n",
    "            word2=B[j]\n",
    "            if confidenceMatrix[i][j]>=min_confidence and score>min_support:\n",
    "                if required.get(word) ==None and (word!='' and word2!='') :\n",
    "                    required[word]=[word2]\n",
    "                else:\n",
    "                    if (word!='' and word2!=''):\n",
    "                        required[word].append(word2)\n",
    "                #print(word,'-->',word2)\n",
    "                #print(\"\\n\")\n",
    "    return required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def MeanOFMean(confidenceMatrix):\\n    mean1=[]\\n    row,col=confidenceMatrix.shape\\n    for i in range(0,row):\\n            mean1.append(np.median(confidenceMatrix[i]))\\n#             print(mean1)\\n    return np.mean(mean1)\\nMeanOFMean(confidenceMatrix)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def MeanOFMean(confidenceMatrix):\n",
    "    mean1=[]\n",
    "    row,col=confidenceMatrix.shape\n",
    "    for i in range(0,row):\n",
    "            mean1.append(np.median(confidenceMatrix[i]))\n",
    "#             print(mean1)\n",
    "    return np.mean(mean1)\n",
    "MeanOFMean(confidenceMatrix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find(string): \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+] |[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) \n",
    "    return url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSummary(rule_Dict,outputfile):\n",
    "    list_of_lines_to_print = []\n",
    "    #print(rule_Dict.keys())\n",
    "    \n",
    "    #print(rule_Dict['personal_information']['DuckDuckGo does not collect or share personal information'])\n",
    "    \n",
    "    #rule_Dict[key][key2] for key2 in rule_Dict[key]\n",
    "    with open(outputfile,\"w\") as file1:\n",
    "        for key in rule_Dict.keys():\n",
    "            testList=rule_Dict[key]\n",
    "            #print([ele[0] for ele in testList])\n",
    "            mean8 = np.mean([ele[0][1] for ele in testList])\n",
    "            #print(mean8)\n",
    "            for rows in testList:\n",
    "                #print(rows[0])\n",
    "                if rows[0][1]>=mean8:\n",
    "                    #rows[0][0]=rows[0][0].decode('utf-8','ignore')\n",
    "                    if rows[0][0] not in list_of_lines_to_print:\n",
    "                        file1.write(rows[0][0]+'\\n\\n')\n",
    "                        list_of_lines_to_print.append(rows[0][0])\n",
    "                    \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def doSummarization(filename,filepath,rules,tfidf_dict):\n",
    "    outputfile = filepath+'summary/'+filename[:-4]+'associationSummary'+'.txt'\n",
    "    filename = filepath+filename\n",
    "    linesData =[]\n",
    "    rule_Dict={}\n",
    "    with open(filename,encoding=\"utf8\") as file:\n",
    "        data = file.readlines()\n",
    "        #print(data)\n",
    "        \n",
    "        for line2 in data:\n",
    "            linedata=line2.split(\".\")\n",
    "            for line in linedata:\n",
    "                s = Find(line)\n",
    "                #print(\"url:\", s)\n",
    "                if len(s)<=0:\n",
    "                    \n",
    "                    line = line.replace(\"\\n\",\"\")\n",
    "                    line = line.replace(\"-\",\"\")\n",
    "                    line1=line.lower()\n",
    "                    #print(\"line:\",line)\n",
    "                    tokens=nltk.word_tokenize(line1)\n",
    "                    if len(tokens)>1:\n",
    "                        tokens=nltk.word_tokenize(line)\n",
    "                        #print(tokens)\n",
    "                        flag=0\n",
    "                        weight=0\n",
    "                        line_dict=[]\n",
    "                        for token in tokens:\n",
    "                            if rules.get(token)!=None:\n",
    "                                for word2 in rules[token]:\n",
    "                                    if word2 in tokens:\n",
    "                                        flag=1\n",
    "                                        #print(\"Printing this line beacuse of \",token,\"-->\",word2)\n",
    "                                        #print('--> ',line,\".\\n\")\n",
    "                                        linesData.append(line)\n",
    "                                        for token1 in tokens:\n",
    "                                            if tfidf_dict.get(token1) is not None:\n",
    "                                                weight=weight+tfidf_dict[token1]\n",
    "                                        line_dict.append([line,weight])\n",
    "                                        break\n",
    "                            if flag==1:\n",
    "                                break\n",
    "                        \n",
    "                        if flag==1:\n",
    "                                w1=token.lower()\n",
    "                                w2=word2.lower()\n",
    "                                str1=w1+'_'+w2\n",
    "                                if rule_Dict.get(str1) is None:\n",
    "                                    rule_Dict[str1]=[line_dict]\n",
    "                                else:\n",
    "                                    rule_Dict[str1].append(line_dict)\n",
    "\n",
    "    #print(len(linesData))\n",
    "   \n",
    "    printSummary(rule_Dict,outputfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    vocab = loadWordDictionaryPickle()\n",
    "    Support_Score={}\n",
    "    DocVocab = loadWordFrequencyPickle()\n",
    "    Word_Set_frequency,wordSetSupport,tfidf_dict=uniqueWordFrequency(DocVocab)\n",
    "    mean1=np.mean([Word_Set_frequency[key]  for key in Word_Set_frequency.keys()])\n",
    "    req_Dic,req_Dic_Sup = filterWordSetFrequencySupport(mean1,Word_Set_frequency,wordSetSupport)\n",
    "    unique_word = loadUniqueWordPickle()\n",
    "    vocab=ListToDic(vocab)\n",
    "    A=getData()\n",
    "    requiredDicti ={}\n",
    "    A,Ascore=FilterDictionary(A,wordSetSupport,unique_word)\n",
    "    A=LemmatizeWord(A)\n",
    "    for key in req_Dic.keys():\n",
    "        if key not in A:\n",
    "            requiredDicti[key]=req_Dic[key]\n",
    "    B=[]\n",
    "    for item,value in requiredDicti.items():\n",
    "        B.append(item)\n",
    "    confidenceMatrix=confidence(A,B,vocab)\n",
    "    print(confidenceMatrix)\n",
    "    rules=SelectAssociationRules(confidenceMatrix,min_support,min_confidence,A,B,wordSetSupport)\n",
    "    test_path = \"Datasets/testdata/\"\n",
    "    test_files = os.listdir(test_path)  \n",
    "    print(test_files)\n",
    "    #doSummarization(\"Datasets/testdata/Github.txt\")\n",
    "    for file in test_files:\n",
    "        if os.path.isdir(test_path+file):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"doing for \",file)\n",
    "            doSummarization(file,test_path,rules,tfidf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(file1,file2,name1,name2):\n",
    "    dict1 = getTokensDictFromFile(file1)\n",
    "    dict2 = getTokensDictFromFile(file2)\n",
    "    sim = getSimilarity(dict1,dict2)\n",
    "    acc = sim*100\n",
    "    print(\"Accuracy for \",name1,\" and \",name2,\" is : \",acc,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "[[1.         0.33333333 0.33333333 ... 0.5        0.         0.5       ]\n",
      " [0.25       0.25       0.         ... 0.5        0.         0.25      ]\n",
      " [0.42105263 0.15789474 0.28947368 ... 0.28947368 0.02631579 0.26315789]\n",
      " ...\n",
      " [1.         0.875      0.375      ... 0.875      0.         0.25      ]\n",
      " [0.375      0.5        0.375      ... 0.25       0.         0.        ]\n",
      " [0.65277778 0.39583333 0.28472222 ... 0.36805556 0.00694444 0.24305556]]\n",
      "['Dubsmash.txt', 'DuckDuckGo.txt', 'Facebook.txt', 'Flickr.txt', 'Github.txt', 'Jabong.txt', 'Loco.txt', 'Microsoft.txt', 'Paytm.txt', 'Random.txt', 'summary', 'Youtube.txt']\n",
      "doing for  Dubsmash.txt\n",
      "doing for  DuckDuckGo.txt\n",
      "doing for  Facebook.txt\n",
      "doing for  Flickr.txt\n",
      "doing for  Github.txt\n",
      "doing for  Jabong.txt\n",
      "doing for  Loco.txt\n",
      "doing for  Microsoft.txt\n",
      "doing for  Paytm.txt\n",
      "doing for  Random.txt\n",
      "doing for  Youtube.txt\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "# sum_path = \"Datasets/testdata/summary/\"\n",
    "# grnd_path =\"Datasets/GroundTruth/\"\n",
    "# sum_files = os.listdir(sum_path)\n",
    "# grnd_files = os.listdir(grnd_path)\n",
    "\n",
    "# #print(sum_files)\n",
    "# #print(grnd_path)\n",
    "# #doSummarization(\"Datasets/testdata/Github.txt\")\n",
    "# for i in range(0,len(sum_files)):\n",
    "#     if os.path.isdir(sum_path+sum_files[i]):\n",
    "#         pass\n",
    "#     else:\n",
    "#         similarity(sum_path+sum_files[i],grnd_path+grnd_files[i],sum_files[i],grnd_files[i])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getTokensDictFromFile(file):\n",
    "    data = open(file).read()\n",
    "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "    tokens = word_tokenize(data.lower().translate(remove_punctuation_map))\n",
    "    words = [word.lower() for word in tokens]\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    stopword = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in stemmed_words if not word in stopword]\n",
    "    \n",
    "    mydict = nltk.defaultdict(int)\n",
    "    for word in filtered_words:\n",
    "        mydict[word]+=1\n",
    "    return mydict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    dot_product = np.dot(a,b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    return dot_product/(norma*normb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(dict1,dict2):\n",
    "    words =[]\n",
    "    for key in dict1.keys():\n",
    "        words.append(key)\n",
    "    for key in dict2.keys():\n",
    "        words.append(key)\n",
    "    n = len(words)\n",
    "    vector1 = np.zeros(n,dtype=np.int)\n",
    "    vector2 = np.zeros(n,dtype=np.int)\n",
    "    i=0\n",
    "    for (key) in words:\n",
    "        vector1[i] = dict1.get(key,0)\n",
    "        vector2[i] = dict2.get(key,0)\n",
    "        i=i+1\n",
    "    sim = cosine_similarity(vector1,vector2)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
